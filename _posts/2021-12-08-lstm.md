---
layout: post
title: 'Long Short-Term Memory'
date: 2021-12-08
author: downeyking
cover: 'https://gitee.com/GoPrime/imagecloud/raw/master/bgcover/lstm.png'
tags: DL

---

> LSTM


#### LSTM的具体计算：

![p23](https://gitee.com/GoPrime/imagecloud/raw/master/img/p23.png)

#### LSTM流程图：

![p25](https://gitee.com/GoPrime/imagecloud/raw/master/img/p25.png)

#### LSTM细节图：

![lstm-detail](https://gitee.com/GoPrime/imagecloud/raw/master/img/lstm-detail.jpg)

#### [词汇表](https://www.cnblogs.com/douzujun/p/13429912.html#_label0)

```
word2ix = {w:i+1 for i,w in enumerate(words)} # 第0是unknown的 所以i+1
ix2word = {i+1:w for i,w in enumerate(words)}
word2ix['<unk>'] = 0
ix2word[0] = '<unk>'
```

#### LSTM维度变化

通过**[源代码](https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM)**中可以看到`nn.LSTM`继承自`nn.RNNBase`,其初始化函数定义如下

```text
class RNNBase(Module):
	...
    def __init__(self, mode, input_size, hidden_size,
                 num_layers=1, bias=True, batch_first=False,
                 dropout=0., bidirectional=False):
```

我们需要关注的参数以及其含义解释如下：

- **input_size** – 输入数据的大小，也就是前面例子中每个单词向量的长度
- **hidden_size** – 隐藏层的大小（即隐藏层节点数量），输出向量的维度等于隐藏节点数
- **num_layers** – recurrent layer的数量，默认等于1。
- **bias** – If False, then the layer does not use bias weights b_ih and b_hh. Default: True
- **batch_first** – 默认为False，也就是说官方不推荐我们把batch放在第一维，这个CNN有点不同，此时输入输出的各个维度含义为 **(seq_length,batch,feature)**。当然如果你想和CNN一样把batch放在第一维，可将该参数设置为True。
- **dropout** – 如果非0，就在除了最后一层的其它层都插入Dropout层，默认为0。
- **bidirectional** – If True, becomes a bidirectional LSTM. Default: False

#### **输入数据**

下面介绍一下输入数据的维度要求(`batch_first=False`)：

输入数据需要按如下形式传入 **input, (h_0,c_0)**

- **input**: 输入数据，即上面例子中的一个句子（或者一个batch的句子），其维度形状为 **(seq_len, batch, input_size)**

- - **seq_len**: 句子长度，即单词数量，这个是需要固定的。当然假如你的一个句子中只有2个单词，但是要求输入10个单词，这个时候可以用`torch.nn.utils.rnn.pack_padded_sequence()`或者`torch.nn.utils.rnn.pack_sequence()`来对句子进行填充或者截断。
  - **batch**：就是你一次传入的句子的数量
  - **input_size**: 每个单词向量的长度，这个必须和你前面定义的网络结构保持一致

- **h_0**：维度形状为 **(num_layers \* num_directions, batch, hidden_size)**:

- - 结合下图应该比较好理解第一个参数的含义**num_layers \* num_directions**， 即LSTM的层数乘以方向数量。这个方向数量是由前面介绍的`bidirectional`决定，如果为False,则等于1；反之等于2。
  - **batch**：同上
  - **hidden_size**: 隐藏层节点数

- **c_0**： 维度形状为 **(num_layers \* num_directions, batch, hidden_size)**,各参数含义和**h_0**类似。

当然，如果你没有传入(h_0, c_0)，那么这两个参数会默认设置为0。

#### **输出数据**

- **output**： 维度和输入数据类似，只不过最后的feature部分会有点不同，即 **(seq_len, batch, num_directions \* hidden_size)**

- - 这个输出tensor包含了LSTM模型最后一层每个time step的输出特征，比如说LSTM有两层，那么最后输出的是![[公式]](https://www.zhihu.com/equation?tex=%5Bh%5E1_0%2Ch%5E1_1%2C...%2Ch%5E1_l%5D),表示第二层LSTM每个time step对应的输出。
  - 另外如果前面你对输入数据使用了`torch.nn.utils.rnn.PackedSequence`,那么输出也会做同样的操作编程packed sequence。
  - 对于unpacked情况，我们可以对输出做如下处理来对方向作分离`output.view(seq_len, batch, num_directions, hidden_size)`, 其中前向和后向分别用0和1表示Similarly, the directions can be separated in the packed case.

- **h_n**：**(num_layers \* num_directions, batch, hidden_size)**，

- - 只会输出最后个time step的隐状态结果（如下图所示）。
  - Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n.

- **c_n** ：**(num_layers \* num_directions, batch, hidden_size)**，只会输出最后个time step的cell状态结果（如下图所示）。

![img](https://i.stack.imgur.com/SjnTl.png)

#### RNN pack相关

[介绍pytorch中pack函数的作用](https://blog.csdn.net/kejizuiqianfang/article/details/100835528#torchnnutilsrnnpad_sequence_28)

[CSDN](https://blog.csdn.net/devil_son1234/article/details/108660887?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link)

[实例](https://www.cnblogs.com/picassooo/p/13577527.html)

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`nn.utils.rnn.PackedSequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence) | Holds the data and list of `batch_sizes` of a packed sequence. |
| [`nn.utils.rnn.pack_padded_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence) | Packs a Tensor containing padded sequences of variable length. |
| [`nn.utils.rnn.pad_packed_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html#torch.nn.utils.rnn.pad_packed_sequence) | Pads a packed batch of variable length sequences.            |
| [`nn.utils.rnn.pad_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html#torch.nn.utils.rnn.pad_sequence) | Pad a list of variable length Tensors with `padding_value`   |
| [`nn.utils.rnn.pack_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_sequence.html#torch.nn.utils.rnn.pack_sequence) | Packs a list of variable length Tensors                      |



#### 参考：

[知乎](https://zhuanlan.zhihu.com/p/100360301)

[知乎](https://zhuanlan.zhihu.com/p/64527432)

